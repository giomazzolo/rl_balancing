

- Swapped from standard deviation to median absolute deviation


Values: [-2, -1, 1, 2]

MAD = 1.5
STD = 1.58





- SOH: internal resistance

In automotive:

Reol = Rbol * 2
Ceol = Cbol * 0.8



Add plots to render function in the environment.



























DQN: 
   + very sample efficient
   - cannot model continuous action spaces.



PPO: 
   + trains relatively fast. Most used of the policy algorithms.
   + Known to be sample-efficient.

New environment.
   Implements continuous action space.
   Discharge multiple cells per step.

Reward: negative Standard Deviation, squared

5 batteries - learns well.
100 batteries - takes longer to figure out what to do.

Add cell aging term. Based on current.
Search paper.

IS RANDOMNESS GOOD? should they start from the same spots?

---------------------------------------------------------------------------------------------------------------------
Next possible improvements

	Action-space normalization - done
	Reward normalization -

	Try different algorithms:
		TRPO should show similar behavior. Same concepts apply.
		A2C
		--! PPO2: combines TRPO and A2C.
			The main idea is that after an update, the new policy should be not too far from the old policy. 
			For that, ppo uses clipping to avoid too large update.
			Problem in reinforcement learning: 
				you can only do 1 step of gradient descent, function used to minimize cost.

			You want to do multiple steps at once, but that makes the policy unstable.
			PPO fixes that by keeping 


		- maybe DDPG. TD3 is is extension which tries to be more stable.
			Combines actor-critic with Q-learning
			- sample inefficient
			- unstable
	
	Hyperparameter optimization: 
                -Currently manual.
		-Automate it.
		Try many hyperparameter combinations and pick the ones that seem to train the fastest.
		Models evaluate models by reward range per episode, for the last 100 or 500 episodes, not by single highest reward.
		Average might work.
		
		Process:
		   - train an amount of models of different random/chosen hyperparameters for X timesteps. 
			  +Evaluate training as well. If a model takes too long to get good results, discard.

	           - run trained models and evaluate them. Pick best one and record the hyperparameters.

		---- computationally expensive.


	New reward, Standard Deviation without squared.


	Data types: float64 
		+ slightly higher precision 
		- more memory required on GPU
		Change to float32 for less memory usage.

	Normalize the action space: [-1, 1], [0, 1]
		x = [ (x - min(x)) / (max(x) - min(x)) - 0.5] * 2


	Changes to make the toy model more similar to the actual simulation:
		Charge+discharge
		
		No time limit. 

		Change action space from 'current' to 'charge rate' formula


	New eval callback? take range or average (think it already uses average of last 100 episodes).
		 
