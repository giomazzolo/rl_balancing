{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Definitions and classification\n",
    "\n",
    "\n",
    "#### <u>Model-based and model-free</u>\n",
    "\n",
    "Model-based means the rl agent has acces to a model of the environment, a function which PREDICTS state transitions and rewards.\n",
    "It allows the agent to plan by thinking ahead, seeing what would happen for a range of possible choices.\n",
    "Mode-based uses this prediction to PLAN AHEAD, simulate outcomes and choose the best action. If our matlab function had the option to rollback to a previous state, we could implement this.\n",
    "Model-learning is fundamentally hard, so even intense effort,being willing to throw lots of time and compute at it,can fail to pay off.\n",
    "\n",
    "\n",
    "Model-free learns directly from experience. \n",
    "Simple and robust but can be data-hungry. Model-free algorithms are effective at learning complex policies, \n",
    "but it takes many trials and can be time-consuming.\n",
    "\n",
    "If the agent can predict the reward for some action before actually performing it thereby planning what it should do, the algorithm is model-based. While if it actually needs to carry out the action to see what happens and learn from it, it is model-free.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### <u>Model-free approaches</u>\n",
    "\n",
    "There are two main approaches to representing and training agents with model-free RL:\n",
    "\n",
    "- Policy Optimization - policy-based: (agent-critic. **A2C** ***A3C***, PPO) ((other RL balancing paper used PPO https://github.com/harwardt/RL-BMS))\n",
    "- Q-learning - value-based: **DQN**, C51(a variation based on DQN), QR-DQN (builds upon C51)\n",
    "- ++ We also aproaches that use both of these in order to balance them out: **DDPG**\n",
    "\n",
    "Examples of Q-learning methods include\n",
    "\n",
    "- **DQN, a classic which substantially launched the field of deep RL**\n",
    "- C51, a variant that learns a distribution over return whose expectation is\n",
    "\n",
    "\n",
    "\n",
    "Example of Policy Optimizations:\n",
    "\n",
    "A3C -  performs gradient ascent to directly maximize performance\n",
    "\n",
    "A3C combines the good parts of both value-based and policy-based, so it’s able to predict both value and policy functions. \n",
    "\n",
    "The agents use the value predicted using the value function to find the optimal policy function. So, the value function acts as a Critic and policy as an Actor. DQN uses experience replay, but also uses more memory and requires more computation. So, A3C suggests using multiple agents in parallel on multiple instances of the environment instead of experiencing replay.\n",
    "\n",
    "PPO -  indirectly maximize performance, by instead maximizing a surrogate objective function \n",
    "\n",
    "Model-free, online (actions affect an environment, live, not a logged set of environment data), on-policy, policy gradient reinforcement learning\n",
    "\n",
    "\n",
    "#### <u>On-policy and off-policy</u>\n",
    "\n",
    "We can say that algorithms classified as on-policy are “learning on the job.” In other words, the algorithm attempts to learn about policy π from experience sampled from π.\n",
    "\n",
    "While algorithms that are classified as off-policy are algorithms that work by “looking over someone’s shoulder.” In other words, the algorithm attempts to learn about policy π from experience sampled from μ. For example, a robot learns how to operate by watching how another human behaves.\n",
    "\n",
    "#### <u>Value-based and policy-based</u>\n",
    "\n",
    "\n",
    "TD (Temporal difference methods):\n",
    "\n",
    "## Q-learning\n",
    " (https://neptune.ai/blog/reinforcement-learning-basics-markov-chain-tree-search)\n",
    "\n",
    "\n",
    "Q Learning is a model-free, online, value-based Reinforcement Algorithm. \n",
    "The focus is on learning the value of an action in a particular state. Two main components help in finding correct action for a given state\n",
    "- Q-table: lookup table where the rows are STATES and columns are ACTIONS\n",
    "- Q-function, Q(s,a): computed for every action and state \n",
    "                        It calculates the values for a decision problem at particular points by using the values from the previous states.\n",
    "\n",
    "\n",
    "Q (st,at) = r(s,a) + max q (st,at)\n",
    "\n",
    "Q (st,at) = Q- value of the action given in a particular state \n",
    "r(s,a) = Reward for taking that action in a given state \n",
    "max q (st,at) = Maximum expected reward in a given state and all the possible actions at that state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### DQN - deep q-network\n",
    "\n",
    "Agent implemeted in tensorflow. \n",
    "\n",
    "https://www.tensorflow.org/agents/api_docs/python/tf_agents/agents/dqn/dqn_agent/DdqnAgent\n",
    "\n",
    "Trained on ATARTI games taking the same visual input,\n",
    "without using problem-specific feature sets. \n",
    "\n",
    "---CAN TAKE A LONG TIME TO TRAIN!!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Policy Optimization\n",
    "\n",
    "\n",
    "These methods offer advantages such as being able to learn stochastic policies which are useful for exploration and dealing with uncertainty, handling continuous action spaces, and being more efficient and stable in high-dimensional or sparse reward scenarios. However, they also have drawbacks such as high variance and slow convergence which require a lot of data and computation, difficulty incorporating prior knowledge or constraints into the policy.\n",
    "\n",
    "\n",
    "\n",
    "-- SARSA (State-action-reward-state-action)  - not many recources found yet\n",
    "Looks at the transition between states.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Comparison Q-learning and Policy optimization\n",
    "\n",
    "Trade-offs Between Policy Optimization and Q-Learning. \n",
    "\n",
    "The primary strength of policy optimization methods is that they are principled, in the sense that you directly optimize for the thing you want. This tends to make them stable and reliable. By contrast, Q-learning methods only indirectly optimize for agent performance, by training Q_{\\theta} to satisfy a self-consistency equation. \n",
    "\n",
    "There are many failure modes for this kind of learning, so it tends to be less stable.\n",
    "\n",
    "But, Q-learning methods gain the advantage of being substantially more sample efficient when they do work, because they can reuse data more effectively than policy optimization techniques."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
